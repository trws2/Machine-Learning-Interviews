# Principles of Large Model Quantization Technology: Summary

In recent years, with the introduction of Transformer and MOE architectures, deep learning models have easily 
surpassed trillions of parameters, leading to increasingly large models. Therefore, we need some large model 
compression techniques to reduce deployment costs and improve inference performance. Model compression can be 
mainly divided into the following categories: model pruning, knowledge distillation, and model quantization. 
This series will discuss some common large model quantization schemes (such as GPTQ, LLM.int8(), SmoothQuant, AWQ, etc.).
